
\documentclass[a4paper,UKenglish,cleveref, autoref, thm-restate]{oasics-v2021}
%This is a template for producing OASIcs articles. 
%See oasics-v2021-authors-guidelines.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
%for section-numbered lemmas etc., use "numberwithinsect"
%for enabling cleveref support, use "cleveref"
%for enabling autoref support, use "autoref"
%for anonymousing the authors (e.g. for double-blind review), add "anonymous"
%for enabling thm-restate support, use "thm-restate"
%for enabling a two-column layout for the author/affilation part (only applicable for > 6 authors), use "authorcolumns"
%for producing a PDF according the PDF/A standard, add "pdfa"

%\pdfoutput=1 %uncomment to ensure pdflatex processing (mandatatory e.g. to submit to arXiv)
%\hideOASIcs %uncomment to remove references to OASIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Towards Modular Compilation using Higher-order Effects}

\author{Jaro S. Reinders}{Delft University of Technology, Netherlands}{j.s.reinders@tudelft.nl}{https://orcid.org/0000-0002-6837-3757}{} % (Optional) author-specific funding acknowledgements}

\authorrunning{J.S. Reinders}

\Copyright{Jaro S. Reinders}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003752.10010124.10010131</concept_id>
<concept_desc>Theory of computation~Program semantics</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011041</concept_id>
<concept_desc>Software and its engineering~Compilers</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Theory of computation~Program semantics}
\ccsdesc[500]{Software and its engineering~Compilers}

\keywords{algebraic effects and handlers, higher-order effects, monadic semantics, modularity, compilation, nanopass}

% \category{} %optional, e.g. invited paper

% \relatedversion{} %optional, e.g. full version hosted on arXiv, HAL, or other respository/website
%\relatedversiondetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93]{Classification (e.g. Full Version, Extended Version, Previous Version}{URL to related version} %linktext and cite are optional

%\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
%\supplementdetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93, subcategory={Description, Subcategory}, swhid={Software Heritage Identifier}]{General Classification (e.g. Software, Dataset, Model, ...)}{URL to related version} %linktext, cite, and subcategory are optional

%\funding{(Optional) general funding statement \dots}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

% \acknowledgements{I want to thank \dots}%optional

\nolinenumbers %uncomment to disable line numbering

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{Ralf L\"{a}mmel, Peter D. Mosses, and Friedrich Steimann}
\EventNoEds{3}
\EventLongTitle{Eelco Visser Commemorative Symposium (EVCS 2023)}
\EventShortTitle{EVCS 2023}
\EventAcronym{EVCS}
\EventYear{2023}
\EventDate{April 5, 2023}
\EventLocation{Delft, The Netherlands}
\EventLogo{}
\SeriesVolume{109}
\ArticleNo{19}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\bind[1]{>\!\!>\!\!= \lambda #1.\,}
\newcommand\then{>\!\!>}
\newcommand\append{+\!\!\!\!+~}


\begin{document}

\maketitle

\begin{abstract}
  Compilers transform a human readable source language into machine readable target language.
  Nanopass compilers simplify this approach by breaking up this transformation into small steps that are more understandable, maintainable, and extensible.
  We propose a semantics-driven variant of the nanopass compiler architecture exploring the use a effects and handlers to model the intermediate languages and the transformation passes, respectively.
  Our approach is fully typed and ensures that all cases in the compiler are covered.
  Additionally, by using an effect system we abstract over the control flow of the intermediate language making the compiler even more flexible.
  We apply this approach to a minimal compiler from a language with arithmetic and let-bound variables to a string of pretty printed X86 instructions.
  In the future, we hope to extend this work to compile a larger and more complicated language and we envision a formal verification framework from compilers written in this style.
\end{abstract}

\section{Introduction}

The essence of a compiler is a function from a source language, suitable for humans, to a machine language, suitable for computers.
As our computers have become more powerful we have seen increasingly complex compilers providing extensive safety guarantees and powerful optimizations.
To manage this complexity, modern compilers are designed as a composition of multiple passes.
However, the total number of passes has traditionally been kept low for the sake of performance, because each pass adds extra overhead.
Thus, compiler passes are more complicated than necessary and therefore harder to \emph{understand}, \emph{maintain}, and \emph{extend}.

To address this problem, Sarkar et al. introduce the nanopass compiler architecture~\cite{10.1145/1016850.1016878}.
In the nanopass architecture, each pass is designed to be as small as possible and has only a single purpose.
To make development of nanopass compilers easier Sarkar et al. proposed a methodology where each pass only has to specify transformation rules for those language elements which they actually modify.
Additionally, intermediate representations of nanopass compilers can be specified by listing language elements which are removed or added to an existing intermediate representation.
To address concerns about the performance of this architecture, Keep and Dybvig have developed a competitive commercial compiler using this architecture~\cite{10.1145/2544174.2500618}.

In this paper, we present our ongoing work on developing an improved nanopass architecture.
As our foundation we use a higher-order effect system~\cite{10.1145/3571255} which is a state of the art technique for modeling the semantics of programming languages with side effects.
Our approach has the practical advantage of preventing type errors in the compiler and ensuring all cases are covered, which means that the passes defined using our architecture are guaranteed to be syntactically correct by construction.
Additionally, our approach abstracts over the control flow giving many of the benefits of continuation-passing-style while retaining a simple monadic interface.

\noindent
Concretely, we make the following contributions:
\begin{itemize}
  \item We introduce a novel approach to designing and implementing practical compilers while staying close to formal denotational semantics specifications (Section~\ref{sec:compiling}). 
  \item We demonstrate our approach on a simple language with arithmetic and let-bound variables by compiling it to a subset of X86 (Section~\ref{sec:compiling}).
\end{itemize}

\section{Monads and Effects}

The compiler architecture we propose is based on the concept of monads, algebraic effects and handlers, and higher-order effect trees and their algebras.
In this section, we briefly introduce this required background.
This section contains no novel work except for the explanations themselves.

\subsection{Monads}

Monads are a concept from category theory that have been applied to the study of programming language semantics by Moggi~\cite{39155} and introduced to functional programming by Wadler~\cite{10.1145/91556.91592}.
A monad as used in this paper consists of four parts: 
\begin{itemize}
  \item a type $m$
  \item a binary operator $>\!\!>\!\!=~: m~a \to (a \to m~b) \to m~b$ (known as ``bind'')
  \item a binary operator $\then~: m~a \to m~b \to m~b$
  \item a function $\mathit{return} : a \to m~a$
\end{itemize}
The operator $x \then y$ is always defined to be equal to $x \bind{z} y$ where $z$ is free in $y$.
Furthermore, monads must satisfy three laws:
\begin{itemize}
  \item left identity: $return~x \bind{y} k~y = k~x$
  \item right identity: $x \bind{y} return~y = x$
  \item associativity: $x \bind{y} (k~y \bind{z} h~z) = (x \bind{y} k~y) \bind{z} h~z$
\end{itemize}

We can use monads is to define computations with side effects.
An example is the \textit{Maybe} monad which defines partial computations.
The type consists of two constructors: $\mathit{Just}: a \to \mathit{Maybe}~a$ and $\mathit{Nothing} : \mathit{Maybe}~a$.
The bind operator sequences partial computations such that if one subcomputation fails then the whole computation fails.
That behavior can be defined using the following two equations: $\mathit{Nothing} \bind{x} k~x = \mathit{Nothing}$ and $\mathit{Just}~x \bind{y} k~y = k~x$.
The return function is the $\mathit{Just}$ constructor.
From these definitions it is straightforward to prove that the laws hold, so we will not show the proofs in this paper.

Using the maybe monad we can define a checked division function assuming we have access to an unchecked division function: 
\[
  \mathit{checkedDiv}~x~y = \begin{cases} 
    \mathit{Just}~(\mathit{div}~x~y) & \text{if}~y \neq 0 \\
    \mathit{Nothing} & \text{if}~y = 0
  \end{cases}
\]

Using this checked division function we can define another function that divides a number $c$ by all the numbers contained in a list and return the result in a list:
\begin{alignat*}{2}
  & \mathit{divAll}~c~\mathit{Nil} && = \mathit{return}~\mathit{Nil} \\
  & \mathit{divAll}~c~(x :: xs) && = 
    \mathit{checkedDiv}~c~x \bind{x'}
    \mathit{divAll}~c~xs \bind{xs'}
    \mathit{return}~(x' :: xs')
\end{alignat*}

In this case, the monad has allowed us to implicitly get the control flow behavior that the whole computation will abort if there is at least one zero in the list.

\subsection{Effects}

One problem with using monads for specifying the semantics of side effects of programming languages is that you need to define a whole new monad for every programming language, even if many programming languages have side effects in common.

Plotkin and Power addressed this problem by introducing algebraic effects~\cite{10.1007/3-540-45315-6_1}, focusing on the effectful operations themselves rather than the concrete monad type.
Any set of effectful operations gives rise to a monad for free.
In this way, the same operations can be reused in two different modular monads.

In this paper we group effectful operations into units which we call 'effects'. For example we can define the interface of an effect called 'Abort' as follows:

\vspace{1em}
\textbf{effect} Abort \textbf{where}
\vspace{-1em}
\begin{alignat*}{1}
  \quad\mathit{abort} & :  m~a
\end{alignat*}

This definition shows that the Abort effect has one operation called 'abort' which takes no arguments.
The type of abort is polymorphic in both the effectful context $m$ and the return type $a$.
In this paper, we assume the obvious implicit constraint that the defining effect, in this case Abort, must be part of the effectful context $m$.

Any combination of effects gives rise to a monad.
So, we can start using the Abort effect to define the \textit{checkedDiv} function instead of using the concrete \textit{Maybe} monad as follows:
\[
  \mathit{checkedDiv}~x~y = \begin{cases} 
    \mathit{return}~(\mathit{div}~x~y) & \text{if}~y \neq 0 \\
    \mathit{abort} & \text{if}~y = 0
  \end{cases}
\]
The implementation of the \textit{divAll} function does not need to be changed.

Now, let us consider the meaning of the Abort effect.
The fact that the \textit{abort} operation claims to produce any polymorphic $a$ as result might seem strange.
A normal interface or type class in most programming languages would not be able to implement such a function, because it is impossible to produce a value if the type of that value is not fixed.
The reason that we are able to define our effectful operation like this is because one possible side effect is to stop the rest of the computation.
That way we do not actually have to produce such a value at all.

In general, to define the meaning of effects we use handlers as introduced by Plotkin and Pretnar~\cite{10.1007/978-3-642-00590-9_7}.
Effect handlers can be thought of as exception handlers with the ability to resume the computation at the location where an effect operation which is handled was used.
For the Abort effect, the usual implementation is defined by the following handler:
\begin{alignat*}{2}
  & \mathbf{handle} && \\
  & \quad\mathit{abort}~k && \to \mathit{Nothing} \\
  & \quad\mathit{return}~x && \to \mathit{Just}~x
\end{alignat*}
This example shows that the handler of an effect lists each operation and how it should be handled.
In addition to the operations themselves, the handler has access to the continuation $k$ from the point where the operation was used.
Furthermore, the Abort handler has a \textit{return} case for when the computation contains no usage of the \textit{abort} operation.
For most handlers in this paper, the \textit{return} case is just $\textit{return}~x \to \textit{return}~x$ in which case we simply leave it out.

Finally, in this paper we use a higher-order effect system introduced by Bach Poulsen and Van der Rest~\cite{10.1145/3571255}.
Higher-order effect operations are those operations that take effectful computations as arguments.
That can be useful for scoping operations, such as exception catching, but also thunks in lazy programming languages.
Bach Poulsen and Van der Rest show that it is possible to use their effect system to define interpreters for such higher-order effects.
So, it is more than expressive enough for the minimal compiler we present in this paper and even we expect it gives us room to expand our compiler in the future.

\section{Compiling with Higher-order Effects}\label{sec:compiling}

In this section, we present our approach by applying it to a very simple language with arithmetic and let-bound variables.
The target language of our compiler is X86 machine code.
We explain the required concepts as we develop our compiler for this language.
The specifications and compiler passes are presented in a simplified notation, but we have implemented all the work we present here in the Agda programming language~\cite{10.1007/978-3-642-03359-9_6}.
Our code can be found on GitHub\footnote{\url{https://github.com/heft-lang/hefty-compilation}}.

We start off by assuming our parser and possibly type checker has finished and produced an abstract syntax tree which follows the grammar described in Figure~\ref{fig:source-absyn}.
Our language has integers, addition, subtraction, negation, a read operation to read an input integer, and a let to bind variables and a var to refer to bound variables.

The abstract syntax is unusual because we reuse the variable binding facilities from our host language in the form of the $\mathit{v} \to \mathit{expr}$ function in the let constructor.
This style of abstract syntax is called parametric higher-order abstract syntax (PHOAS) \cite{10.1145/1411203.1411226}.
It allows us to avoid the complexities of variable binding and thus simplify our presentation.
We believe other name binding approaches, such as De Bruijn indices~\cite{DEBRUIJN1972381}, could be used instead.
However, changing the name binding mechanism would also require changing our representation of effectful computations.

\begin{figure}[ht]
  \begin{align*}
    \mathit{expr} ::= &\ \mathrm{int}(n) \\
           | &\ \mathrm{add}(\mathit{expr},\mathit{expr}) \\
           | &\ \mathrm{sub}(\mathit{expr},\mathit{expr}) \\
           | &\ \mathrm{neg}(\mathit{expr}) \\
           | &\ \mathrm{read} \\
           | &\ \mathrm{let}(\mathit{expr}, \mathit{v} \to \mathit{expr}) \\
           | &\ \mathrm{var}(\mathit{v})
  \end{align*}
  \caption{Abstract syntax of our simple language with arithmetic and let-bound variables.}\label{fig:source-absyn}
\end{figure}

The first step of our compilation pipeline will be to denote these syntactic constructs onto an effectful computation.
We have chosen to divide our source language into four effects: Int, Arith, Read, and Let.

\begin{figure}[ht]
\begin{minipage}[t]{0.4\textwidth}
\textbf{effect} Int \textbf{where}
\vspace{-1em}
\begin{alignat*}{2}
  \mathit{int} & : \mathbb{Z} && \to m~\mathit{val}
\end{alignat*}

\textbf{effect} Arith \textbf{where}
\vspace{-1em}
\begin{alignat*}{2}
  \mathit{add} & : \mathit{val} \to \mathit{val} && \to m~\mathit{val} \\
  \mathit{sub} & : \mathit{val} \to \mathit{val} && \to m~\mathit{val} \\
  \mathit{neg} & : \mathit{val}                  && \to m~\mathit{val}
\end{alignat*}
\end{minipage}%
%
\begin{minipage}[t]{0.6\textwidth}
\textbf{effect} Read \textbf{where}
\vspace{-1em}
\begin{alignat*}{2}
  \mathit{read} & : m~\mathit{val}
\end{alignat*}

\textbf{effect} Let \textbf{where}
\vspace{-1em}
\begin{alignat*}{2}
  \mathit{let} & : m~\mathit{val} \to (\mathit{val} \to m~\mathit{val}) && \to m~\mathit{val}
\end{alignat*}
\end{minipage}
  \caption{The effects of our source language and their operations with type signatures.}\label{fig:source-ops}
\end{figure}

Figure~\ref{fig:source-ops} shows the operations that correspond directly to our source language.

To keep our example simple, we have chosen to use this single type for all values, but we keep the type abstract and simply call it `val'.
Also note that we now need a special $\mathit{int}$ operation to inject integers into this abstract value type.

The reason for keeping the value type abstract is twofold. 
Firstly, this prevents us from accidentally attempting to use information from these run-time values at compile-time. 
Secondly, at the end of the compilation, these values stand for register or memory locations.
Keeping the values abstract gives us the freedom to choose the concrete representation later on in the compilation pipeline.
We use this freedom in the last handler of this section where we convert the program to a string.
There we choose the values to be strings.

The first pass of our compiler pipeline maps our abstract syntax from Figure~\ref{fig:source-absyn} onto the operations we have defined for our source language from Figure~\ref{fig:source-ops}.
This mapping, called a denotation and written using the $[\![ \cdot ]\!]$ notation, is a recursive traversal of the abstract syntax tree shown in Figure~\ref{fig:den}.
The result of this mapping is a monadic computation involving the Int, Arith, Read, and Let effects.

\begin{figure}[ht]
\begin{align*}
  [\![ \mathrm{int}(n) ]\!] & = \mathit{int}~n \\
  [\![ \mathrm{add}(e_1, e_2) ]\!] & = [\![ e_1 ]\!] \bind{x} [\![ e_2 ]\!] \bind{y} \mathit{add}~x~y \\
  [\![ \mathrm{sub}(e_1, e_2) ]\!] & = [\![ e_1 ]\!] \bind{x} [\![ e_2 ]\!] \bind{y} \mathit{sub}~x~y \\
  [\![ \mathrm{neg}(e) ]\!] & = [\![ e ]\!] \bind{x} \mathit{neg}~x \\
  [\![ \mathrm{read} ]\!] & = \mathit{read} \\
  [\![ \mathrm{let}(e,f) ]\!] & = \mathit{let}~[\![e]\!]~(\lambda x.\, [\![ f~x ]\!]) \\
  [\![ \mathrm{var}(x) ]\!] & = \mathit{return}~x
\end{align*}
  \caption{A denotational mapping from our abstract syntax onto our initial set of effectful operations.}\label{fig:den}
\end{figure}

Now that we have denoted our syntactic elements into our semantic domain as operations, we can start refining these operations to get closer to the desired target language which is X86 in our case.
The effect that we choose to handle first is the Let effect, which only has the $\mathit{let}$ operation.
We handle this operation by running the right hand side of the binding, passing the resulting value to the body, and finally passing the result of that to the continuation. In code that looks as follows:
\begin{align*}
  \mathbf{handle}~(\mathit{let}~e~f)~k & \to e \bind{x} f~x \bind{z} k~z
\end{align*}
Note that this defines a strict semantics for our let bindings.
By using a different handler we could give different semantics to our language.
This is an example of the flexibility of algebraic effects and handlers.

\begin{figure}[ht]
  \begin{minipage}[t]{0.4\textwidth}
  \textbf{effect} X86 \textbf{where}
  \vspace{-1em}
  \begin{alignat*}{3}
    & \mathit{addq}  && : \mathit{val} \to \mathit{val} && \to m~() \\
    & \mathit{subq}  && : \mathit{val} \to \mathit{val} && \to m~() \\
    & \mathit{negq}  && : \mathit{val}                  && \to m~() \\
    & \mathit{movq}  && : \mathit{val} \to \mathit{val} && \to m~() \\
    & \mathit{callq} && : \mathit{lab}                  && \to m~() \\
    & \mathit{reg}   && : \mathrm{Register}             && \to m~\mathit{val} \\
    & \mathit{deref} && : \mathrm{Register} \to \mathit{\mathbb{Z}} && \to m~\mathit{val}
  \end{alignat*}
  \end{minipage}
  \begin{minipage}[t]{0.4\textwidth}
  \textbf{effect} X86Var \textbf{where}
  \vspace{-1em}
  \begin{align*}
    \mathit{x86var} : m~\mathit{val}
  \end{align*}
  \end{minipage}
  \caption{The effects related to X86 and their operations with type signatures.}\label{fig:x86-ops}
\end{figure}

At this point, since our language is so simple we can already begin translating into our target language.
In Figure~\ref{fig:x86-ops} we show a minimal subset of X86 that we need to compile our Arith and Read effects.
This subset contains in-place arithmetic instructions, the ubiquitous move instruction, the call instruction, and an operation to inject concrete registers into our abstract value type.
Additionally, we add an operation to generate fresh variables and inject them into our abstract value type.

We can translate our Arith effect operations into X86 operations by creating a fresh X86 variable, populating it, and then applying the in-place arithmetic operation to the variable.
So, we write handler as follows:
\begin{alignat*}{2}
  & \mathbf{handle} && \\
  & \quad(\mathit{add}~x~y) &&~k \to \mathit{x86var} \bind{z} \mathit{movq}~x~z \then \mathit{addq}~y~z \then k~z \\
  & \quad(\mathit{sub}~x~y) &&~k \to \mathit{x86var} \bind{z} \mathit{movq}~x~z \then \mathit{subq}~y~z \then k~z \\
  & \quad(\mathit{neg}~x) &&~k \to \mathit{x86var} \bind{z} \mathit{movq}~x~z \then \mathit{negq}~z \then k~z
\end{alignat*}
The $\mathit{read}$ operation requires us to call a function that we will assume is defined in a standard library called $\mathrm{read\_int}$. This function places its output in the $\mathrm{\%rax}$ register, so we have to move it to avoid it being overwritten by other parts of our program.
The full definition of our handler for the Read effect is as follows:
\begin{align*}
  \mathbf{handle}~\mathit{read}~k & \to \mathit{x86var} \bind{z} \mathit{callq}~\mathrm{read\_int} \then \mathit{reg}~\mathrm{\%rax} \bind{x} \mathit{movq}~x~z \then k~z
\end{align*}

The final challenge to complete this minimal compiler pipeline is to allocate the X86 variables on the stack.
Conceptually, this requires us to give each $\mathit{x86var}$ operation and give each its own location of the stack.
Keeping track of such information in our handler, however, is something we have not yet needed to do for the passes up to this point.
Until now, we have handled each effect by translating into other effects directly.
Instead, we can parameterize our handlers which means we pass along an extra parameter while handling our operations.
Parameterized handlers take one extra parameter and need to pass one extra argument to the continuation\footnote{We ignore effectful subcomputations, because they were already removed in an earlier pass.}. 
Now we can write the parameterized handler for the X86Var effect which assigns each variable to its own stack location as follows:
\begin{align*}
  \mathbf{handle}~\mathit{x86var}~k~n \to \mathit{deref}~\mathrm{\%rbp}~(-8 \cdot n) \bind{z} k~z~(n + 1)
\end{align*}
Note that we assume sufficient space is allocated on the stack.
Additionally, when applying this handler we need to provide the starting value of the parameter $n$, which we will choose to be $1$.

At this point, we have a full compiler pipeline from our source language to a subset of X86, but is still in the form of an effectful computation.
To get a concrete representation, we implement two handlers for the remaining Int and X86 effects to produce an output string.
As part of choosing this concrete representation, we also choose the concrete type for the variables $\mathit{val}$ and $\mathit{lab}$ to be the string type.
We define the handler that turns our effectful computation of Int and X86 effects into a concrete string representation as follows:
\begin{alignat*}{2}
  & \mathbf{handle} && \\
  & \quad(\mathit{int}~n)~&& k \to k~(\mathrm{showInt}~n) \\
  & \quad(\mathit{addq}~x~y)~&& k \to \texttt{"addq "} \append x \append \texttt{", "} \append y \append \texttt{"\textbackslash{}n"} \append k~() \\
  & \quad(\mathit{subq}~x~y)~&& k \to \texttt{"subq "} \append x \append \texttt{", "} \append y \append \texttt{"\textbackslash{}n"} \append k~() \\
  & \quad(\mathit{negq}~x)~&& k \to \texttt{"negq "} \append x \append \texttt{"\textbackslash{}n"} \append k~() \\
  & \quad(\mathit{movq}~x~y)~&& k \to \texttt{"movq "} \append x \append \texttt{", "} \append y \append \texttt{"\textbackslash{}n"} \append k~() \\
  & \quad(\mathit{callq}~l)~&& k \to \texttt{"callq "} \append l \append \texttt{"\textbackslash{}n"} \append k~() \\
  & \quad(\mathit{reg}~r)~&& k \to k~(\mathrm{showReg}~r) \\
  & \quad(\mathit{deref}~r~n)~&& k \to k~(\mathrm{showInt}~n \append \texttt{"("} \append \mathrm{showReg}~r \append \texttt{")"}) \\
  & \quad\mathit{return}~x~&& \hspace{0.55em}\to x 
\end{alignat*}

\section{Related Work}\label{sec:related}

Our work is influenced by Eelco Visser's work on the Spoofax Language Workbench~\cite{10.1145/1869459.1869497}. 
Eelco Visser was the original designer of the Stratego program transformation language~\cite{10.1007/3-540-45127-7_27} which is part of Spoofax. 
Stratego can be used to implement a whole compiler back end, however Spoofax was still lacking a way to specifying programming language semantics at a higher level of abstraction.

One step in that direction by Vlad Vergu, Pierre Neron, and Eelco Visser was the DynSem DSL for dynamic semantics specification~\cite{vergu_et_al:LIPIcs:2015:5208}.
Later, Vlad Vergu and Eelco Visser developed a way to improve the performance of programs written in languages specified in DynSem by using just-in-time compilation~\cite{10.1145/3237009.3237018}.
However, they were not able to fully eliminate the interpretation overhead that DynSem imposes.
Furthermore, DynSem uses big-step operational reduction rules, which require explicit managing of the control flow.
For languages which include complicated control flow constructs, such as exceptions, this requires non-trivial glue code in every reduction rule.

In the meantime, Eelco Visser started working on adding type systems to Spoofax.
Firstly, together with Van Antwerpen et al., he developed Statix~\cite{10.1145/3276484}, which is a language for defining the static semantics of languages defined in Spoofax.
Later, Eelco Visser and Jeff Smits added a gradual type system to the meta-language Stratego~\cite{10.1145/3426425.3426928}.

Recently, Thijs Molendijk, who was supervised by Eelco Visser, has developed the Dynamix~\cite{dynamix} dynamic semantics specification language for Spoofax.
Dynamix is more amenable to compilation and it allows for specifying the semantics of complicated control flow constructs independently from other language constructs.
The monadic style of Dynamix makes it similar to our work, but the main difference is that Dynamix has a fixed intermediate representation where the only way to extend it is to add new primitives.

As mentioned in the introduction, our approach embraces the nanopass architecture~\cite{10.1145/1016850.1016878, 10.1145/2544174.2500618}.
The main idea of nanopass compilers is that they consist of many small single purpose passes, which aids understanding.
We improve upon this work by making it fully typed to prevent common errors and even check that all cases are covered, and by abstracting over the control flow in the compiler.

\section{Conclusions and Future Work}\label{sec:conclusion}

We have presented a new semantics-driven approach to writing compilers by using effect operations as an intermediate representation.
We use effect handlers to iteratively refine operations in terms of increasingly lower level operations to finally reach a target machine language.

We have shown a concrete example of this approach applied to a very simple language with arithmetic and let-bound variables.
This example application consists of an implementation of a denotation function and handlers which compile this language is compiled in several passes to X86 machine language.

In the future, we would like to extend this minimal compiler with more complicated language constructs such as conditionals, exceptions, and anonymous functions.

Additionally, we would like to implement more complicated analyses on this effectful representation, such as register allocation.
We expect these analyses to consist of two stages: first derive concrete structures such as control-flow graphs and interference graphs from our effectful representation, and then perform a pass over the effectful computation that uses the results of the analysis over these structures to transform the program.
The first stage would be similar to our handler that turns the effectful computation into a concrete string and the second stage would employ a parameterized handler similar to our stack allocation handler.

Furthermore, we would like to explore the verification of our compilers using algebraic laws for our effect operations, inspired by Interaction Trees~\cite{10.1145/3371119}.
To be specific, we can define a set of laws that describe the behavior of each of the effects in our compiler pipeline.
If these laws are sound and complete, with respect to for example definitional interpreters for the effects, then we can prove compiler correctness by proving that these laws are preserved by each of our handlers.


%%
%% Bibliography
%%

%% Please use bibtex, 

\bibliography{references}

\end{document}
